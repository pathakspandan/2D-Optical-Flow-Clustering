{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b45a7c16-72c4-4054-9ad6-d6465a3d28c7",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "016f113a-20df-4795-b68b-5759867cc213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, cv2, csv\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cupyx.scipy import ndimage as cpx_ndimage  # Import CuPy's GPU ndimage module\n",
    "import tifffile as tiff\n",
    "import scipy as sp\n",
    "from scipy import ndimage, io as sio\n",
    "from scipy.ndimage import maximum_filter, label, find_objects\n",
    "from scipy.stats import chi2, lognorm, poisson, norm\n",
    "from scipy.optimize import curve_fit\n",
    "from skimage.feature import peak_local_max\n",
    "from natsort import natsorted\n",
    "import trackpy as tp\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.widgets import Slider\n",
    "from matplotlib import animation, rc\n",
    "import matplotlib.colors as mcolors\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Set up matplotlib for animations\n",
    "rc('animation', html='jshtml')\n",
    "\n",
    "# Suppress specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # ignore warnings for specific matplotlib commands which will be outdated soon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f33380-c4fd-457b-93ad-e850fb05d0b7",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90383a21-e9b8-4146-a5f2-63c7870c94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_code_path(windows_path):\n",
    "    \"\"\"\n",
    "    Converts a Windows file path with single backslashes to a format with double backslashes for use in Python code.\n",
    "\n",
    "    Parameters:\n",
    "    - windows_path: str\n",
    "        The original Windows file path (e.g., 'C:\\\\Users\\\\YourName\\\\Folder').\n",
    "\n",
    "    Returns:\n",
    "    - str\n",
    "        The modified path with double backslashes, suitable for Python (e.g., 'C:\\\\\\\\Users\\\\\\\\YourName\\\\\\\\Folder').\n",
    "    \"\"\"\n",
    "    # Replace each single backslash with a double backslash\n",
    "    return windows_path.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "\n",
    "\n",
    "\n",
    "def cluster_optical_flow(us, vs, kernel_size=11, kernel_type='gaussian', normalize=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Applies a convolution to horizontal (u) and vertical (v) components of optical flow data and clusters the flow.\n",
    "\n",
    "    Parameters:\n",
    "    - us: list of 2D numpy arrays\n",
    "        List of horizontal components of optical flow (u) for each frame.\n",
    "    - vs: list of 2D numpy arrays\n",
    "        List of vertical components of optical flow (v) for each frame.\n",
    "    - kernel_type: str, optional, default='gaussian'\n",
    "        Type of kernel to use for clustering. Options are 'gaussian' or 'uniform'.\n",
    "    - kernel_size: int, optional, default=11\n",
    "        Size of the kernel (must be odd).\n",
    "    - normalize: bool, optional, default=False\n",
    "        Whether to normalize the u and v vectors to avoid magnitude differences.\n",
    "    - verbose: bool, optional, default=False\n",
    "        If True, shows progress during computation.\n",
    "\n",
    "    Returns:\n",
    "    - cluster: list of 2D numpy arrays\n",
    "        List of clustered optical flow arrays for each frame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the kernel size is odd\n",
    "    if kernel_size % 2 == 0:\n",
    "        print(kernel_size)\n",
    "        raise ValueError(\"kernel_size must be an odd integer.\")\n",
    "    \n",
    "    # Create the kernel based on the specified type\n",
    "    N = kernel_size\n",
    "    if kernel_type == 'gaussian':\n",
    "        gaussian_window = sp.signal.windows.gaussian(N, std=5, sym=True).reshape(-1, 1)\n",
    "        gaussian_kernel = gaussian_window @ gaussian_window.T\n",
    "        gaussian_kernel[N // 2, N // 2] = 0  # Set the center value to 0 to avoid self-weighting\n",
    "        kernel_gpu = cp.asarray(gaussian_kernel)  # Convert to CuPy array for GPU usage\n",
    "\n",
    "    elif kernel_type == 'uniform':\n",
    "        kernel_gpu = cp.ones((N, N), dtype=cp.float32)\n",
    "        kernel_gpu[N // 2, N // 2] = 0  # Center weight is set to zero\n",
    "        kernel_gpu = kernel_gpu / cp.sum(kernel_gpu)  # Normalize the kernel\n",
    "\n",
    "    else:\n",
    "        print(kernel_type)\n",
    "        raise ValueError(\"Unsupported kernel_type. Choose 'gaussian' or 'uniform'.\")\n",
    "\n",
    "    # List to store the clustered results\n",
    "    cluster = []\n",
    "\n",
    "    # Iterate over each u, v optical flow component\n",
    "    for u, v in tqdm(zip(us, vs), disable=(not verbose)):\n",
    "        u_gpu = cp.asarray(u)  # Move u to GPU\n",
    "        v_gpu = cp.asarray(v)  # Move v to GPU\n",
    "\n",
    "        # Normalize the u and v flow vectors if required\n",
    "        if normalize:\n",
    "            norm = cp.sqrt(u_gpu**2 + v_gpu**2)  # Calculate the magnitude of flow\n",
    "            norm[norm == 0] = 1.0  # Prevent division by zero for zero flow vectors\n",
    "            u_gpu = u_gpu / norm\n",
    "            v_gpu = v_gpu / norm\n",
    "            u_gpu[cp.isnan(u_gpu)] = 0  # Handle NaNs after normalization\n",
    "            v_gpu[cp.isnan(v_gpu)] = 0\n",
    "\n",
    "        # Convolve the flow fields using GPU-accelerated convolution\n",
    "        u_conv = cpx_ndimage.convolve(u_gpu, kernel_gpu, mode='constant', cval=0.0)\n",
    "        v_conv = cpx_ndimage.convolve(v_gpu, kernel_gpu, mode='constant', cval=0.0)\n",
    "\n",
    "        # Multiply the original flow fields by the convolved result\n",
    "        u_gpu = u_gpu * u_conv\n",
    "        v_gpu = v_gpu * v_conv\n",
    "\n",
    "        # Append the result (u + v) to the cluster list and move data back to CPU\n",
    "        cluster.append(cp.asnumpy(u_gpu + v_gpu))\n",
    "\n",
    "    return cluster\n",
    "\n",
    "\n",
    " \n",
    "def process_optical_flow(data_folder, file_name, kernel_type='gaussian', kernel_size=11, \n",
    "                         normalize=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to process optical flow and cluster the results.\n",
    "    \n",
    "    Args:\n",
    "    - data_folder: Path to the folder containing the data.\n",
    "    - file_name: Name of the .tiff image file.\n",
    "    - mask_type: The type of mask to load ('neural_mask' or 'cell_mask'). Default is 'neural_mask'.\n",
    "    - kernel_type: Type of kernel to use in clustering. Default is 'gaussian'.\n",
    "    - kernel_size: Size of the kernel to use in clustering. Default is 11.\n",
    "    - normalize: Boolean flag to indicate whether to normalize the flow. Default is False.\n",
    "    - verbose: Boolean flag for verbosity. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    - cluster: The clustered optical flow data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define paths\n",
    "    image_path = os.path.join(data_folder, file_name)\n",
    "    of_path = os.path.join(data_folder, 'Op_flow')\n",
    "\n",
    "    # Load the image\n",
    "    img = tiff.imread(image_path)\n",
    "\n",
    "    # Load the mask\n",
    "    mask_path = os.path.join(data_folder, 'neural_mask.mat')\n",
    "    mask = sio.loadmat(mask_path)['neural_mask']\n",
    "    mask = mask.astype(float)\n",
    "    mask[mask == 0] = np.NaN\n",
    "\n",
    "    try:\n",
    "        # Get the optical flow file list\n",
    "        of_list = natsorted(os.listdir(of_path))\n",
    "    \n",
    "        # Get number of frames and span dimensions from the first optical flow file\n",
    "        n_frames = len(of_list)\n",
    "    \n",
    "        # Check the condition for the type of optical flow file (.mat or .npz)\n",
    "        if '.mat' in of_list[0]:\n",
    "            y_span = np.shape(sio.loadmat(of_path+'\\\\0.mat')['vy'])[0]\n",
    "            x_span = np.shape(sio.loadmat(of_path+'\\\\0.mat')['vy'])[1]\n",
    "            file_flag = 0\n",
    "        elif '.npz' in of_list[0]:\n",
    "            y_span = np.shape(np.load(of_path+'\\\\0.npz')['vy'])[0]\n",
    "            x_span = np.shape(np.load(of_path+'\\\\0.npz')['vy'])[1]\n",
    "            file_flag = 1\n",
    "        else:\n",
    "            print ('Unknown file type found!')\n",
    "    \n",
    "        # Initialize arrays to store velocity fields\n",
    "        vy_all = np.zeros((n_frames, y_span, x_span))\n",
    "        vx_all = np.zeros((n_frames, y_span, x_span))\n",
    "    \n",
    "        # Load all velocity fields\n",
    "        if file_flag == 0:\n",
    "            for i in tqdm(range(n_frames), desc=\"Loading optical flow data\"):\n",
    "                flow_data = sio.loadmat(os.path.join(of_path, of_list[i]))\n",
    "                vy_all[i, :, :] = flow_data['vy']\n",
    "                vx_all[i, :, :] = flow_data['vx']\n",
    "            \n",
    "        elif file_flag == 1:\n",
    "            for i in tqdm(range(n_frames), desc=\"Loading optical flow data\"):\n",
    "                flow_data = np.load(os.path.join(of_path, of_list[i]))\n",
    "                vy_all[i, :, :] = flow_data['vy']\n",
    "                vx_all[i, :, :] = flow_data['vx']\n",
    "    \n",
    "        # Cluster the optical flow (assuming `cluster_optical_flow` is predefined)\n",
    "        cluster = cluster_optical_flow(vx_all, vy_all, kernel_size=kernel_size, kernel_type=kernel_type, normalize=normalize, \n",
    "                                       verbose=verbose)\n",
    "    \n",
    "        return cluster, mask, n_frames\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def detect_and_track_particles(cluster, mask, n_frames, diameter=5, minmass=1.0, separation=15, search_range=5):\n",
    "    \"\"\"\n",
    "    Detects particles in each frame of the clustered optical flow data and tracks them across frames.\n",
    "    \n",
    "    Args:\n",
    "    - cluster: 3D array of clustered optical flow data.\n",
    "    - mask: 2D array mask to apply to each frame (NaNs are applied as zero).\n",
    "    - n_frames: Number of frames in the 3D stack.\n",
    "    - diameter: Approximate size of the particle in pixels. Default is 5.\n",
    "    - minmass: Minimum integrated brightness (mass) of a particle to be considered. Default is 1.0.\n",
    "    - separation: Minimum separation between particles in pixels. Default is 15.\n",
    "    - search_range: Maximum displacement between frames in pixels for tracking. Default is 5.\n",
    "    \n",
    "    Returns:\n",
    "    - tp_trajectories: DataFrame with tracked particle positions over time.\n",
    "    \"\"\"\n",
    "    \n",
    "    # A list to store particle locations for each frame\n",
    "    particle_positions = []\n",
    "\n",
    "    # Loop through each 2D frame to detect particles\n",
    "    for t in tqdm(range(n_frames), desc=\"Detecting particles\"):\n",
    "        frame = np.nan_to_num(cluster[t] * mask, nan=0)  # Get the 2D frame from the 3D stack and apply mask\n",
    "        \n",
    "        # Detect particles with sub-pixel accuracy in this frame\n",
    "        particles = tp.locate(frame, diameter=diameter, minmass=minmass, separation=separation)\n",
    "        \n",
    "        # Add the frame number to each detected particle's data\n",
    "        particles['frame'] = t\n",
    "        \n",
    "        # Store the detected particles\n",
    "        particle_positions.append(particles)\n",
    "\n",
    "    # Combine all particle positions across frames into a single DataFrame\n",
    "    peaks_df = pd.concat(particle_positions, ignore_index=True)\n",
    "\n",
    "    # Use trackpy to link the particles over time\n",
    "    tp_trajectories = tp.link(peaks_df, search_range=search_range, memory=1)\n",
    "\n",
    "    return tp_trajectories\n",
    "\n",
    "\n",
    "\n",
    "def visualize_and_save_trajectories(tp_trajectories_filtered, img, data_folder, figsize=(12, 12), dpi=600):\n",
    "    \"\"\"\n",
    "    Visualize filtered particle trajectories superimposed on an image and save the plot as a high-resolution image.\n",
    "\n",
    "    Args:\n",
    "    - tp_trajectories_filtered: DataFrame containing filtered trajectories with 'x', 'y', and 'particle' columns.\n",
    "    - img: The image array (e.g., the first frame of the time-lapse) to use as a background.\n",
    "    - data_folder: Path to the folder where the output image will be saved.\n",
    "    - filename: Name of the file to save the image as. Default is 'filtered_tracks.png' or 'filtered_tracks.svg'.\n",
    "    - figsize: Tuple specifying the figure size. Default is (8, 12).\n",
    "    - dpi: Resolution of the saved image. Default is 600.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Plot the trajectories on top of the provided image\n",
    "    tp.plot_traj(tp_trajectories_filtered, superimpose=img[0], ax=ax)\n",
    "\n",
    "    # Set equal axis scaling\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Display the plot\n",
    "    # plt.show()\n",
    "\n",
    "    # Save the figure as an image file in the specified folder\n",
    "    fig.savefig(f'{data_folder}\\\\filtered_tracks.svg', format='svg', bbox_inches='tight', dpi=dpi)\n",
    "    fig.savefig(f'{data_folder}\\\\filtered_tracks.png', format='png', bbox_inches='tight', dpi=dpi)\n",
    "\n",
    "    print (\"Tracks plotted\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def plot_particle_trajectories(tp_trajectories_filtered, data_folder, colormap_name='jet', scatter_size=80, alpha=0.5, \n",
    "                               line_color='gray', dpi=600):\n",
    "    \"\"\"\n",
    "    Plots particle trajectories as scatter points with progressive colors and dashed lines connecting them.\n",
    "    \n",
    "    Args:\n",
    "    - tp_trajectories_filtered: DataFrame containing filtered trajectories with 'x', 'y', 'particle', and 'frame' columns.\n",
    "    - data_folder: Path to save the plot as an SVG file.\n",
    "    - colormap_name: Name of the colormap to use for progressive coloring. Default is 'jet'.\n",
    "    - scatter_size: Size of the scatter points. Default is 80.\n",
    "    - alpha: Transparency level of scatter points. Default is 0.5.\n",
    "    - line_color: Color of the dashed lines connecting the points. Default is 'grey'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    colormap = cm.get_cmap(colormap_name)\n",
    "\n",
    "    # Set figure background color to grey\n",
    "    ax.set_facecolor('grey')\n",
    "\n",
    "    # Group by particle ID to plot each track separately with progressive colors\n",
    "    for particle_id, track in tp_trajectories_filtered.groupby('particle'):\n",
    "        # Normalize frame values to use for progressive color mapping\n",
    "        norm = mcolors.Normalize(vmin=track['frame'].min(), vmax=track['frame'].max())\n",
    "\n",
    "        # Plot each point with a color based on its frame\n",
    "        for i in range(1, len(track)):\n",
    "            x_values = [track.iloc[i-1]['x'], track.iloc[i]['x']]\n",
    "            y_values = [track.iloc[i-1]['y'], track.iloc[i]['y']]\n",
    "            color = colormap(norm(track.iloc[i]['frame']))  # Map frame to color\n",
    "            \n",
    "            # Plot segment\n",
    "            ax.plot(x_values, y_values, linestyle='--', linewidth=1, color=color)\n",
    "\n",
    "            # Scatter plot for each point with progressive color\n",
    "            ax.scatter(track.iloc[i]['x'], track.iloc[i]['y'], s=scatter_size, facecolor='none', alpha=alpha, color=color)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    ax.set_xlabel('X Position')\n",
    "    ax.set_ylabel('Y Position')\n",
    "    ax.set_title('Particle Trajectories with Progressive Colors and Dashed Lines')\n",
    "\n",
    "    # Ensure equal axis scaling\n",
    "    ax.set_aspect('equal')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Save the figure as SVG with high DPI\n",
    "    fig.savefig(f'{data_folder}\\\\scatter_tracks.svg', format='svg', bbox_inches='tight', dpi=dpi)\n",
    "    print (\"Tracks plotted with progressive colors\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def calculate_track_metrics(tp_trajectories_filtered, resolution, frame_interval):\n",
    "    \"\"\"\n",
    "    Calculate and store instantaneous shifts, displacements, velocities, total track properties, \n",
    "    and other metrics for each particle's trajectory.\n",
    "\n",
    "    Parameters:\n",
    "    - tp_trajectories_filtered (pandas DataFrame): Particle tracking data with 'x', 'y', 'frame', and 'particle' columns.\n",
    "    - resolution (float): Spatial resolution to convert units as required (e.g., pixels to microns).\n",
    "    - frame_interval (float): Time interval between frames to calculate velocities and track duration.\n",
    "\n",
    "    Returns:\n",
    "    - tp_trajectories_filtered (pandas DataFrame): Input DataFrame with added columns:\n",
    "        - 'x_shift': x-direction shift per frame for each particle.\n",
    "        - 'y_shift': y-direction shift per frame for each particle.\n",
    "        - 'frame_shift': Number of frames between positions in the trajectory.\n",
    "        - 'displacement': Frame-to-frame displacement (Euclidean distance) in pixels.\n",
    "        - 'inst_velocity': Instantaneous velocity per frame (#px./frame) for each particle.\n",
    "    - track_properties (pandas DataFrame): Aggregated track-level metrics per particle, including:\n",
    "        - 'track_length': Total track length, i.e., cumulative distance traveled by each particle (microns).\n",
    "        - 'track_duration': Total duration of each particle's trajectory (seconds).\n",
    "        - 'distance': Net displacement from initial to final position (microns).\n",
    "        - 'avg_velocity': Average net velocity (net displacement divided by total time, in microns/min).\n",
    "        - 'inst_velocity': Average instantaneous velocity (total track length divided by total time, in microns/min).\n",
    "        - 'sinuosity': Measure of track straightness (track length divided by net displacement).\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate Instantaneous Shifts (x, y, and frame)\n",
    "    tp_trajectories_filtered['x_shift'] = (tp_trajectories_filtered.groupby('particle')['x'].diff().fillna(0)) \n",
    "    tp_trajectories_filtered['y_shift'] = (tp_trajectories_filtered.groupby('particle')['y'].diff().fillna(0)) \n",
    "    tp_trajectories_filtered['frame_shift']  = tp_trajectories_filtered.groupby('particle')['frame'].diff().fillna(1)\n",
    "    \n",
    "    # Calculate instantaneous displacement and velocity\n",
    "    tp_trajectories_filtered['displacement'] = (np.sqrt(tp_trajectories_filtered['x_shift']**2 + tp_trajectories_filtered['y_shift']**2))\n",
    "    tp_trajectories_filtered['displacement_scaled'] = (np.sqrt(tp_trajectories_filtered['x_shift']**2 + tp_trajectories_filtered['y_shift']**2)) / resolution\n",
    "    tp_trajectories_filtered['inst_velocity'] = (tp_trajectories_filtered['displacement'] /\n",
    "                                                 tp_trajectories_filtered['frame_shift'])\n",
    "    \n",
    "    # Drop NaN values from displacement and velocity\n",
    "    tp_trajectories_filtered.dropna(subset=['displacement', 'inst_velocity'], inplace=True)\n",
    "\n",
    "\n",
    "    # Calculate the direction (angle) of movement at each time step\n",
    "    tp_trajectories_filtered['angle'] = np.arctan2(-tp_trajectories_filtered['y_shift'], tp_trajectories_filtered['x_shift'])\n",
    "    \n",
    "    # Handle zero shifts (no movement)\n",
    "    tp_trajectories_filtered['angle'] = np.where(\n",
    "        (tp_trajectories_filtered['x_shift'] == 0) & (tp_trajectories_filtered['y_shift'] == 0), np.nan,\n",
    "        tp_trajectories_filtered['angle']\n",
    "    )\n",
    "    \n",
    "    # Calculate track-level metrics: total distance traveled (track length), track duration, distance, average velocity, and sinuosity   \n",
    "    \n",
    "    # Calculate track length (total distance traversed) by summing displacements for each particle\n",
    "    track_properties = tp_trajectories_filtered.groupby('particle')['displacement'].sum().reset_index()   \n",
    "    track_properties['track_length'] = track_properties['displacement'] / resolution\n",
    "    \n",
    "    # Dropping the 'displacement' column from the new dataframe\n",
    "    track_properties = track_properties.drop(columns='displacement')\n",
    "    \n",
    "    # Calculate track duration (total frames converted to seconds)\n",
    "    track_duration = tp_trajectories_filtered.groupby('particle')['frame'].nunique().reset_index(name='track_duration')\n",
    "    track_duration['track_duration'] = (track_duration['track_duration']-1) * frame_interval\n",
    "    \n",
    "    # Merge the track_duration back into track_properties DataFrame\n",
    "    track_properties = track_properties.merge(track_duration[['particle', 'track_duration']], on='particle', how='left')\n",
    "\n",
    "    # Calculate total displacement (net distance between start and end positions)\n",
    "    track_metrics = tp_trajectories_filtered.groupby('particle').agg(\n",
    "        x_start=('x', 'first'),\n",
    "        y_start=('y', 'first'),\n",
    "        x_end=('x', 'last'),\n",
    "        y_end=('y', 'last'),\n",
    "        frame_start=('frame', 'first'),\n",
    "        frame_end=('frame', 'last')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate total displacement (net distance between start and end positions)\n",
    "    track_properties['distance'] = np.sqrt((track_metrics['x_end'] - track_metrics['x_start'])**2 +\n",
    "                                            (track_metrics['y_end'] - track_metrics['y_start'])**2) / resolution\n",
    "    \n",
    "    # Calculate average net speed (net displacement / total time)\n",
    "    track_properties['avg_velocity'] = track_properties['distance'] * 60 / track_properties['track_duration']\n",
    "    \n",
    "    # Calculate the average instantaneous velocity (track length / total time)\n",
    "    track_properties['inst_velocity'] =  track_properties['track_length'] * 60 / track_properties['track_duration']\n",
    "\n",
    "    # Calculate sinuosity (track length / net displacement)\n",
    "    track_properties['sinuosity'] = track_properties['track_length'] / track_properties['distance']\n",
    "    \n",
    "    # Now track_metrics contains 'distance', 'avg_velocity', 'avg_inst_velocity', and 'sinuosity'\n",
    "\n",
    "    # Filter for tracks with average instantaneous velocity > 0.5 pixels/frame\n",
    "    track_properties_thresholded = track_properties[track_properties['inst_velocity'] > 0.5 * 60 / (resolution * frame_interval)]\n",
    "    \n",
    "    # Log a success message\n",
    "    print(\"Trajectories and track properties calculated and stored.\")\n",
    "    \n",
    "    # Return filtered results\n",
    "    return tp_trajectories_filtered, track_properties_thresholded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_angular_differences_process(tp_trajectories_final, data_folder, save_path=None):\n",
    "    \"\"\"\n",
    "    Calculate angular differences between trajectory start angles and process orientation\n",
    "    from the 'LoG_orientation.mat' file.\n",
    "\n",
    "    Parameters:\n",
    "    tp_trajectories_final (DataFrame): DataFrame containing trajectory start coordinates and angles.\n",
    "    data_folder (str): Path to the folder containing the 'LoG_orientation.mat' file.\n",
    "    save_path (str, optional): If provided, save the results as a CSV file to the specified path.\n",
    "\n",
    "    Returns:\n",
    "    ang_diff_process (array): Array of angular differences for start angles (wrt process orientation).\n",
    "    \"\"\"\n",
    "    # Load process orientation from the 'LoG_orientation.mat' file (process orientation is a matrix)\n",
    "    mat_file_process = os.path.join(data_folder, 'LoG_orientation.mat')\n",
    "    data_process = sio.loadmat(mat_file_process)\n",
    "    process_angle_array = data_process['prefAng']  # Process orientation in degrees (matrix)\n",
    "\n",
    "    # Initialize array to store process orientations for the starting coordinates\n",
    "    ang_start_arr_process = np.zeros(len(tp_trajectories_final['x']))\n",
    "\n",
    "    # Extract process orientations for the starting coordinates of trajectories\n",
    "    for i in range(len(tp_trajectories_final['x'])):\n",
    "        x_start = round(tp_trajectories_final['x'].iloc[i])\n",
    "        y_start = round(tp_trajectories_final['y'].iloc[i])\n",
    "\n",
    "        # Retrieve the process orientation at the trajectory's starting position\n",
    "        ang_start_arr_process[i] = process_angle_array[y_start, x_start]\n",
    "\n",
    "    # Convert trajectory angles to degrees\n",
    "    angle_values_deg = np.rad2deg(tp_trajectories_final['angle'].values)  # Convert from radians to degrees\n",
    "\n",
    "    # Calculate angular differences with respect to process orientation\n",
    "    ang_diff_process = np.abs(ang_start_arr_process - angle_values_deg)\n",
    "\n",
    "    # Normalize angular differences to [0, 180] and then collapse to [0, 90]\n",
    "    ang_diff_process = np.mod(ang_diff_process, 180)  # Normalize to [0, 180]\n",
    "    ang_diff_process = np.where(ang_diff_process > 90, 180 - ang_diff_process, ang_diff_process)\n",
    "\n",
    "    # Create a DataFrame to save the results\n",
    "    df = pd.DataFrame({\n",
    "        'ang_diff_process': ang_diff_process\n",
    "    })\n",
    "\n",
    "    # If a save path is provided, save the DataFrame to a CSV file\n",
    "    if save_path:\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "    # Return the angular differences array and the DataFrame\n",
    "    return ang_diff_process\n",
    "\n",
    "\n",
    "\n",
    "def plot_angular_differences_process(ang_diff_process, tp_trajectories_final, transparency=0.9, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot the angular differences for process orientations on a polar plot with transparency.\n",
    "\n",
    "    Parameters:\n",
    "    ang_diff_process (array): Array of angular differences for process orientation.\n",
    "    tp_trajectories_final (DataFrame): DataFrame containing trajectory information, including displacement.\n",
    "    transparency (float): Transparency level for the histograms (default: 0.9).\n",
    "    save_path (str, optional): Path to save the plot (default is None, no saving).\n",
    "    \"\"\"\n",
    "    # Remove NaN values from the angular difference array and corresponding displacement values\n",
    "    valid_indices_process = ~np.isnan(ang_diff_process)\n",
    "    ang_diff_process = ang_diff_process[valid_indices_process]\n",
    "    displacement_process = tp_trajectories_final['displacement'].iloc[valid_indices_process]\n",
    "\n",
    "    # Create a polar plot\n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(8, 8))\n",
    "\n",
    "    # Convert angular differences to radians for the polar plot\n",
    "    ang_diff_process_rad = np.deg2rad(ang_diff_process)\n",
    "\n",
    "    # Define number of bins\n",
    "    bins = 30\n",
    "\n",
    "    # Calculate the histogram for process orientation angular differences\n",
    "    n_process, bins_process = np.histogram(ang_diff_process_rad, bins=bins)\n",
    "\n",
    "    # Normalize to fraction of counts within each bin (i.e., divide by the total count)\n",
    "    n_process = n_process / np.sum(n_process)\n",
    "\n",
    "    # Plot angular differences for process orientation as fraction of counts\n",
    "    ax.hist(ang_diff_process_rad, bins=bins_process, weights=displacement_process, alpha=transparency, color='red',\n",
    "            label='wrt Neuronal Process', histtype='step', density=False)\n",
    "\n",
    "    # Set angular limits for the plot between 0 and 90 degrees\n",
    "    ax.set_thetamin(0)  # Minimum angle (0 degrees)\n",
    "    ax.set_thetamax(90)  # Maximum angle (90 degrees)\n",
    "\n",
    "    # Customize the plot title and labels\n",
    "    ax.set_title(\"Relative Orientation of Tracks wrt Process\", fontsize=16)\n",
    "    ax.set_ylabel(\"Counts\", fontsize=14)\n",
    "\n",
    "    # Customize the legend\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.5, 1), fontsize=14)\n",
    "\n",
    "    # Increase the font size for the ticks\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()  # Adjust layout to prevent label overlap\n",
    "    \n",
    "    # If a save path is provided, save the plot as a .svg file\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, format='svg')\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define model functions\n",
    "def power_law(x, a, k):\n",
    "    return a * np.power(x, k)\n",
    "\n",
    "def exponential_decay(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "\n",
    "def gaussian(x, amplitude, mean, stddev):\n",
    "    \"\"\"\n",
    "    Gaussian function.\n",
    "\n",
    "    Args:\n",
    "    - x: Input data points.\n",
    "    - amplitude: Height of the peak of the Gaussian.\n",
    "    - mean: Position of the center of the peak.\n",
    "    - stddev: Standard deviation (width of the peak).\n",
    "\n",
    "    Returns:\n",
    "    - y: The Gaussian function evaluated at x.\n",
    "    \"\"\"\n",
    "    return amplitude * np.exp(-((x - mean) ** 2) / (2 * stddev ** 2))\n",
    "\n",
    "\n",
    "def lognormal(x, shape, loc, scale):\n",
    "    \"\"\"Returns the value of the log-normal probability density function.\"\"\"\n",
    "    return lognorm.pdf(x, shape, loc=loc, scale=scale)\n",
    "\n",
    "\n",
    "def analyze_track_duration(data_folder, tp_trajectories_final):\n",
    "    \"\"\"\n",
    "    Analyze track duration distribution by fitting power-law and exponential decay models.\n",
    "    \n",
    "    Parameters:\n",
    "    - tp_trajectories_final (pd.DataFrame): DataFrame containing track data with 'track_duration' column in seconds.\n",
    "    - data_folder (str): Folder path to save the plot and results.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Fit results containing parameters and R² values for power-law and exponential fits.\n",
    "    \"\"\"\n",
    "    # Calculate histogram for track duration\n",
    "    counts, bin_edges = np.histogram(\n",
    "        tp_trajectories_final['track_duration'], \n",
    "        bins = np.arange(15, 40, 2), \n",
    "        density=False\n",
    "    )\n",
    "\n",
    "    # Calculate bin centers and normalize counts\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    total_counts = np.sum(counts)\n",
    "    normalized_counts = counts / total_counts if total_counts > 0 else counts\n",
    "\n",
    "    # Initialize fit result dictionary\n",
    "    fit_results = {\n",
    "        \"power_law\": {\"params\": None, \"errors\": None, \"r2\": None},\n",
    "        \"exponential\": {\"params\": None, \"errors\": None, \"r2\": None}\n",
    "    }\n",
    "\n",
    "    # Fit the power-law and exponential decay to the histogram data\n",
    "    try:\n",
    "        # Filter out zero counts for fitting\n",
    "        valid_bins = bin_centers[counts > 0]\n",
    "        valid_norm_counts = normalized_counts[counts > 0]\n",
    "\n",
    "        # Fit power-law model\n",
    "        popt_power, pcov_power = curve_fit(\n",
    "            power_law, valid_bins, valid_norm_counts, maxfev=10000\n",
    "        )\n",
    "        perr_power = np.sqrt(np.diag(pcov_power))\n",
    "\n",
    "        # Calculate R² for power-law fit\n",
    "        power_pred = power_law(bin_centers, *popt_power)\n",
    "        ss_res_power = np.sum((normalized_counts - power_pred) ** 2)\n",
    "        ss_tot_power = np.sum((normalized_counts - np.mean(normalized_counts)) ** 2)\n",
    "        r2_power = 1 - (ss_res_power / ss_tot_power)\n",
    "\n",
    "        # Store power-law fit results\n",
    "        fit_results[\"power_law\"][\"params\"] = popt_power\n",
    "        fit_results[\"power_law\"][\"errors\"] = perr_power\n",
    "        fit_results[\"power_law\"][\"r2\"] = r2_power\n",
    "\n",
    "        # Fit exponential decay model with bounds to avoid overflow\n",
    "        popt_exp, pcov_exp = curve_fit(\n",
    "            exponential_decay, valid_bins, valid_norm_counts, bounds=(0, [np.inf, 1.0]), maxfev=10000\n",
    "        )\n",
    "        perr_exp = np.sqrt(np.diag(pcov_exp))\n",
    "\n",
    "        # Calculate R² for exponential fit\n",
    "        exp_pred = exponential_decay(bin_centers, *popt_exp)\n",
    "        ss_res_exp = np.sum((normalized_counts - exp_pred) ** 2)\n",
    "        ss_tot_exp = np.sum((normalized_counts - np.mean(normalized_counts)) ** 2)\n",
    "        r2_exp = 1 - (ss_res_exp / ss_tot_exp)\n",
    "\n",
    "        # Store exponential fit results\n",
    "        fit_results[\"exponential\"][\"params\"] = popt_exp\n",
    "        fit_results[\"exponential\"][\"errors\"] = perr_exp\n",
    "        fit_results[\"exponential\"][\"r2\"] = r2_exp\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error fitting curves: {e}\")\n",
    "\n",
    "    # Plot Track Duration Distribution\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plot normalized histogram\n",
    "    ax.bar(\n",
    "        bin_centers, normalized_counts, width=np.diff(bin_edges), \n",
    "        color='wheat', edgecolor='k', alpha=0.7, \n",
    "        label='Normalized Duration Histogram'\n",
    "    )\n",
    "\n",
    "    # Plot fitted power-law curve\n",
    "    if fit_results[\"power_law\"][\"params\"] is not None:\n",
    "        ax.plot(\n",
    "            bin_centers, power_law(bin_centers, *fit_results[\"power_law\"][\"params\"]), marker='o', \n",
    "            color='olive', label=f'Power-law Fit: y = {fit_results[\"power_law\"][\"params\"][0]:.2f} * \\\n",
    "            x^{fit_results[\"power_law\"][\"params\"][1]:.2f}'\n",
    "        )\n",
    "\n",
    "    # Plot fitted exponential curve\n",
    "    if fit_results[\"exponential\"][\"params\"] is not None:\n",
    "        ax.plot(\n",
    "            bin_centers, exponential_decay(bin_centers, *fit_results[\"exponential\"][\"params\"]), marker='o', \n",
    "            color='slateblue', label=f'Exponential Fit: y = {fit_results[\"exponential\"][\"params\"][0]:.2f} *exp(-{fit_results[\"exponential\"][\"params\"][1]:.2f} * x)'\n",
    "        )\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_xlabel('Track Duration (s)', fontsize=20)\n",
    "    ax.set_ylabel('Normalized Counts', fontsize=20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=8)\n",
    "    ax.grid(False)\n",
    "    ax.legend(fontsize=16)\n",
    "\n",
    "    # Save the plot\n",
    "    save_path = os.path.join(data_folder, 'track_durations.svg')\n",
    "    if os.path.exists(save_path):\n",
    "        os.remove(save_path)\n",
    "    fig.savefig(save_path, format='svg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Save fit results to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Distribution\": [\"Power-law\", \"Exponential\"],\n",
    "        \"Parameters\": [\n",
    "            fit_results[\"power_law\"][\"params\"],\n",
    "            fit_results[\"exponential\"][\"params\"]\n",
    "        ],\n",
    "        \"Uncertainties\": [\n",
    "            fit_results[\"power_law\"][\"errors\"],\n",
    "            fit_results[\"exponential\"][\"errors\"]\n",
    "        ],\n",
    "        \"R^2\": [\n",
    "            fit_results[\"power_law\"][\"r2\"],\n",
    "            fit_results[\"exponential\"][\"r2\"]\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    csv_path = os.path.join(data_folder, 'fit_track_duration_parameters.csv')\n",
    "    if os.path.exists(csv_path):\n",
    "        os.remove(csv_path)\n",
    "\n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "    \n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # return fit_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_track_total_distance_distribution(data_folder, tp_trajectories_final):\n",
    "    \"\"\"\n",
    "    Plots the track distance distribution as a histogram of normalized counts \n",
    "    and fits log-normal and Poisson distributions to the data.\n",
    "\n",
    "    Parameters:\n",
    "    - tp_trajectories_final (pd.DataFrame): DataFrame containing track data with 'track_length' column.\n",
    "    - data_folder (str): The folder path where the plot and results will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Fit results containing parameters and R² values for log-normal and Poisson fits.\n",
    "    \"\"\"\n",
    "    # Use track lengths directly, assuming they are already in microns\n",
    "    track_lengths = tp_trajectories_final['track_length']\n",
    "\n",
    "    # Define histogram bins from 0 to 25 µm\n",
    "    bins = np.linspace(0, 25, 20 + 1)  # Adjust bin range as necessary\n",
    "    counts, bin_edges = np.histogram(track_lengths, bins=bins, density=False)\n",
    "    normalized_counts = counts / np.sum(counts) if np.sum(counts) > 0 else counts\n",
    "\n",
    "    # Calculate bin centers for plotting\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.bar(bin_centers, normalized_counts, width=np.diff(bin_edges), color='lavender', edgecolor='k', \n",
    "           align='center', label='Normalized Counts')\n",
    "\n",
    "    # Fit results dictionary to store parameters and R² values\n",
    "    fit_results = {\n",
    "        \"log_normal\": {\"params\": None, \"errors\": None, \"r2\": None},\n",
    "        \"poisson\": {\"params\": None, \"r2\": None}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Log-normal fit\n",
    "        shape, loc, scale = lognorm.fit(track_lengths, floc=0)  # Fit log-normal, fix location to 0\n",
    "        lognorm_pdf = lognorm.pdf(bin_centers, shape, loc, scale)\n",
    "        lognorm_pdf /= np.sum(lognorm_pdf)  # Normalize\n",
    "\n",
    "        # Calculate R² for log-normal fit\n",
    "        ss_res_lognorm = np.sum((normalized_counts - lognorm_pdf) ** 2)\n",
    "        ss_tot_lognorm = np.sum((normalized_counts - np.mean(normalized_counts)) ** 2)\n",
    "        r2_lognorm = 1 - (ss_res_lognorm / ss_tot_lognorm)\n",
    "\n",
    "        # Store log-normal fit results and uncertainties (estimated using standard errors)\n",
    "        fit_results[\"log_normal\"][\"params\"] = (shape, loc, scale)\n",
    "        fit_results[\"log_normal\"][\"errors\"] = (np.sqrt(np.diag(lognorm.fit(\n",
    "            track_lengths, floc=0, scale=scale, loc=loc))),)  # Example uncertainty calculation\n",
    "        fit_results[\"log_normal\"][\"r2\"] = r2_lognorm\n",
    "\n",
    "        # Plot log-normal fit\n",
    "        ax.plot(bin_centers, lognorm_pdf, color='teal', lw=2, marker='o', label=f'Log-normal Fit (R² = {r2_lognorm:.4f})')\n",
    "\n",
    "        # Poisson fit (parameter is the mean of the track lengths in bins)\n",
    "        poisson_lambda = np.mean(track_lengths)\n",
    "        poisson_pmf = poisson.pmf(bin_centers.astype(int), poisson_lambda)\n",
    "        poisson_pmf /= np.sum(poisson_pmf)  # Normalize\n",
    "\n",
    "        # Calculate R² for Poisson fit\n",
    "        ss_res_poisson = np.sum((normalized_counts - poisson_pmf) ** 2)\n",
    "        ss_tot_poisson = np.sum((normalized_counts - np.mean(normalized_counts)) ** 2)\n",
    "        r2_poisson = 1 - (ss_res_poisson / ss_tot_poisson)\n",
    "\n",
    "        # Store Poisson fit results\n",
    "        fit_results[\"poisson\"][\"params\"] = (poisson_lambda,)\n",
    "        fit_results[\"poisson\"][\"r2\"] = r2_poisson\n",
    "\n",
    "        # Plot Poisson fit\n",
    "        ax.plot(bin_centers, poisson_pmf, color='crimson', lw=2, marker='o', label=f'Poisson Fit (R² = {r2_poisson:.4f})')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting distributions: {e}\")\n",
    "\n",
    "    # Set plot title and labels\n",
    "    ax.set_xlabel('Track Distance (µm)', fontsize=20)\n",
    "    ax.set_ylabel('Normalized Distance Counts', fontsize=20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=8)\n",
    "    ax.legend(fontsize=16)\n",
    "\n",
    "    # Save as SVG\n",
    "    save_path = os.path.join(data_folder, 'track_total_distance.svg')\n",
    "    if os.path.exists(save_path):\n",
    "        os.remove(save_path)\n",
    "    fig.savefig(save_path, format='svg', bbox_inches='tight')\n",
    "    # print(f\"Plot saved as SVG at: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save fit results to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Distribution\": [\"Log-normal\", \"Poisson\"],\n",
    "        \"Parameters\": [fit_results[\"log_normal\"][\"params\"], fit_results[\"poisson\"][\"params\"]],\n",
    "        \"Uncertainties\": [fit_results[\"log_normal\"][\"errors\"], None],\n",
    "        \"R^2\": [fit_results[\"log_normal\"][\"r2\"], fit_results[\"poisson\"][\"r2\"]]\n",
    "    })\n",
    "\n",
    "    csv_path = os.path.join(data_folder, 'fit_total_distance_parameters.csv')\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        os.remove(csv_path)\n",
    "\n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "    \n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    # print(f\"Fit results saved as CSV at: {csv_path}\")\n",
    "\n",
    "    # return fit_results\n",
    "\n",
    "\n",
    "\n",
    "def fit_and_plot_track_displacement_distribution(data_folder, tp_trajectories_final):\n",
    "    \"\"\"\n",
    "    Plots the track distance distribution and fits power-law and exponential decay curves.\n",
    "    \n",
    "    Parameters:\n",
    "    - tp_trajectories_final (pd.DataFrame): DataFrame containing track data with 'distance' column.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Fit results containing parameters, errors, and R² values for power-law and exponential fits.\n",
    "    \"\"\"\n",
    "    #  Use track distances directly, assuming they are already in microns\n",
    "    distances = tp_trajectories_final['distance']\n",
    "\n",
    "    # Define histogram bins from 0 to 7 µm\n",
    "    nbins = 20\n",
    "    bins = np.arange(0, 7 + 7 / nbins, 7 / nbins)\n",
    "    counts, bin_edges = np.histogram(distances, bins=bins, density=False)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    normalized_counts = counts / np.sum(counts) if np.sum(counts) > 0 else counts\n",
    "\n",
    "    # Fit results dictionary to store parameters, errors, and R² values\n",
    "    fit_results = {\n",
    "        \"power_law\": {\"params\": None, \"errors\": None, \"r2\": None},\n",
    "        \"exponential\": {\"params\": None, \"errors\": None, \"r2\": None}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Fit power-law to non-zero counts\n",
    "        popt_power, pcov_power = curve_fit(power_law, bin_centers[counts > 0], normalized_counts[counts > 0], maxfev=10000)\n",
    "        perr_power = np.sqrt(np.diag(pcov_power))  # Standard deviation errors\n",
    "\n",
    "        # Calculate R² for power-law fit\n",
    "        power_pred = power_law(bin_centers, *popt_power)\n",
    "        ss_res_power = np.sum((normalized_counts - power_pred) ** 2)\n",
    "        ss_tot_power = np.sum((normalized_counts - np.mean(normalized_counts)) ** 2)\n",
    "        r2_power = 1 - (ss_res_power / ss_tot_power)\n",
    "\n",
    "        # Store power-law fit results\n",
    "        fit_results[\"power_law\"][\"params\"] = popt_power\n",
    "        fit_results[\"power_law\"][\"errors\"] = perr_power\n",
    "        fit_results[\"power_law\"][\"r2\"] = r2_power\n",
    "\n",
    "        # Fit exponential decay to non-zero counts\n",
    "        popt_exp, pcov_exp = curve_fit(exponential_decay, bin_centers[counts > 0], normalized_counts[counts > 0], \n",
    "                                       maxfev=10000)\n",
    "        perr_exp = np.sqrt(np.diag(pcov_exp))  # Standard deviation errors\n",
    "\n",
    "        # Calculate R² for exponential fit\n",
    "        exp_pred = exponential_decay(bin_centers, *popt_exp)\n",
    "        ss_res_exp = np.sum((normalized_counts - exp_pred) ** 2)\n",
    "        ss_tot_exp = np.sum((normalized_counts - np.mean(normalized_counts)) ** 2)\n",
    "        r2_exp = 1 - (ss_res_exp / ss_tot_exp)\n",
    "\n",
    "        # Store exponential fit results\n",
    "        fit_results[\"exponential\"][\"params\"] = popt_exp\n",
    "        fit_results[\"exponential\"][\"errors\"] = perr_exp\n",
    "        fit_results[\"exponential\"][\"r2\"] = r2_exp\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error fitting curves: {e}\")\n",
    "\n",
    "    # Plot Track Final Distance Distribution\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plot normalized histogram\n",
    "    ax.bar(bin_centers, normalized_counts, width=np.diff(bin_edges), color='azure', edgecolor='k', alpha=0.7, \n",
    "           label='Normalized Displacement Histogram')\n",
    "\n",
    "    # Plot power-law fit\n",
    "    if fit_results[\"power_law\"][\"params\"] is not None:\n",
    "        ax.plot(bin_centers, power_law(bin_centers, *fit_results[\"power_law\"][\"params\"]), color='olive', marker='o',\n",
    "                label=f'Power-law Fit: y = {fit_results[\"power_law\"][\"params\"][0]:.2f} * x^{fit_results[\"power_law\"]\n",
    "                [\"params\"][1]:.2f}')\n",
    "\n",
    "    # Plot exponential fit\n",
    "    if fit_results[\"exponential\"][\"params\"] is not None:\n",
    "        ax.plot(bin_centers, exponential_decay(bin_centers, *fit_results[\"exponential\"][\"params\"]), \n",
    "                color='slateblue', marker='o',\n",
    "                label=f'Exponential Fit: y = {fit_results[\"exponential\"][\"params\"][0]:.2f} *exp(-{fit_results[\"exponential\"][\"params\"][1]:.2f} * x)')\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_xlabel('Track Final Displacement (µm)', fontsize=20)\n",
    "    ax.set_ylabel('Counts per Bin / Total Bins', fontsize=20)  # Update y-label\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=8)\n",
    "    ax.grid(False)\n",
    "    ax.legend(fontsize=16)\n",
    "\n",
    "    # Show plot\n",
    "    # plt.show()\n",
    "\n",
    "    # Save as SVG\n",
    "    save_path = data_folder + '\\\\track_final_distance.svg'\n",
    "    if os.path.exists(save_path):\n",
    "        os.remove(save_path)\n",
    "    fig.savefig(save_path, format='svg', bbox_inches='tight')\n",
    "    # print(f\"Plot saved as SVG at: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save fit parameters and R² values to CSV\n",
    "    fit_data = {\n",
    "        'Fit Type': ['Power-law', 'Exponential'],\n",
    "        'Parameter a': [fit_results[\"power_law\"][\"params\"][0] if fit_results[\"power_law\"][\"params\"] is not None else None,\n",
    "                        fit_results[\"exponential\"][\"params\"][0] if fit_results[\"exponential\"][\"params\"] is not None else None],\n",
    "        'Error a': [fit_results[\"power_law\"][\"errors\"][0] if fit_results[\"power_law\"][\"errors\"] is not None else None,\n",
    "                     fit_results[\"exponential\"][\"errors\"][0] if fit_results[\"exponential\"][\"errors\"] is not None else None],\n",
    "        'Parameter k/b': [fit_results[\"power_law\"][\"params\"][1] if fit_results[\"power_law\"][\"params\"] is not None else None,\n",
    "                          fit_results[\"exponential\"][\"params\"][1] if fit_results[\"exponential\"][\"params\"] is not None else None],\n",
    "        'Error k/b': [fit_results[\"power_law\"][\"errors\"][1] if fit_results[\"power_law\"][\"errors\"] is not None else None,\n",
    "                       fit_results[\"exponential\"][\"errors\"][1] if fit_results[\"exponential\"][\"errors\"] is not None else None],\n",
    "        'R²': [fit_results[\"power_law\"][\"r2\"], fit_results[\"exponential\"][\"r2\"]]\n",
    "    }\n",
    "\n",
    "    fit_df = pd.DataFrame(fit_data)\n",
    "    fit_csv_path = data_folder + '\\\\fit_displacement_parameters.csv'\n",
    "\n",
    "    if os.path.exists(fit_csv_path):\n",
    "            os.remove(fit_csv_path)\n",
    "\n",
    "    with open(fit_csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "    \n",
    "    \n",
    "    fit_df.to_csv(fit_csv_path, index=False)\n",
    "    # print(f\"Fit parameters saved to {fit_csv_path}\")\n",
    "    # return fit_results\n",
    "\n",
    "\n",
    "\n",
    "def save_fit_velocity_parameters_as_csv(gaussian_params, gaussian_r2, lognormal_params, lognormal_r2, filename):\n",
    "    \"\"\"\n",
    "    Saves the fitting parameters for velocity distributions to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - gaussian_params (list): List of parameters for the Gaussian fit.\n",
    "    - gaussian_r2 (float): R² value for the Gaussian fit.\n",
    "    - lognormal_params (list): List of parameters for the log-normal fit.\n",
    "    - lognormal_r2 (float): R² value for the log-normal fit.\n",
    "    - filename (str): Path to the CSV file to save the parameters.\n",
    "    \"\"\"\n",
    "    parameters_data = {\n",
    "        'Fit Type': [],\n",
    "        'Parameter': [],\n",
    "        'Value': [],\n",
    "        'R²': []\n",
    "    }\n",
    "\n",
    "    # Adding Gaussian fit parameters\n",
    "    for i, param in enumerate(gaussian_params):\n",
    "        parameters_data['Fit Type'].append('Gaussian')\n",
    "        parameters_data['Parameter'].append(f'Parameter {i+1}')\n",
    "        parameters_data['Value'].append(param)\n",
    "        parameters_data['R²'].append(gaussian_r2 if gaussian_r2 is not None else '')\n",
    "\n",
    "    # Adding log-normal fit parameters\n",
    "    for i, param in enumerate(lognormal_params):\n",
    "        parameters_data['Fit Type'].append('Log-normal')\n",
    "        parameters_data['Parameter'].append(f'Parameter {i+1}')\n",
    "        parameters_data['Value'].append(param)\n",
    "        parameters_data['R²'].append(lognormal_r2 if lognormal_r2 is not None else '')\n",
    "\n",
    "    # Create DataFrame and save to CSV\n",
    "    parameters_df = pd.DataFrame(parameters_data)\n",
    "    parameters_df.to_csv(filename, index=False)\n",
    "\n",
    "    \n",
    "\n",
    "def plot_velocity_distributions(data_folder, tp_trajectories_final):\n",
    "    # Convert velocities to desired units (μm/min)\n",
    "    avg_inst_velocity = tp_trajectories_final['avg_inst_velocity'] \n",
    "    avg_velocity = tp_trajectories_final['avg_velocity'] \n",
    "    \n",
    "    # Define fixed bins for both histograms\n",
    "    nbins = 20\n",
    "    bins_smaller = np.arange(0, 25 + (25 - 5) / nbins, (25 - 5) / nbins)\n",
    "    bins_bigger = np.arange(0, 40 + (40 - 0) / nbins, (40 - 5) / nbins)\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Calculate normalized counts for average instantaneous velocity distribution\n",
    "    counts_inst, bins_inst = np.histogram(avg_inst_velocity, bins=bins_bigger)\n",
    "    counts_inst_normalized = counts_inst / counts_inst.sum()\n",
    "    ax.hist(bins_inst[:-1], bins=bins_bigger, weights=counts_inst_normalized, color='palevioletred', \n",
    "            alpha=0.5, label='Average Speed')\n",
    "    \n",
    "    # Fit Gaussian to the average instantaneous velocity\n",
    "    bin_centers_inst = 0.5 * (bins_inst[1:] + bins_inst[:-1])\n",
    "    params_inst, _ = curve_fit(gaussian, bin_centers_inst, counts_inst_normalized, \n",
    "                               p0=[1, np.mean(avg_inst_velocity), np.std(avg_inst_velocity)])\n",
    "    \n",
    "    # Generate x values for the fitted Gaussian\n",
    "    x_fit_inst = np.linspace(bins_inst[0], bins_inst[-1], 100)\n",
    "    y_fit_inst = gaussian(x_fit_inst, *params_inst)\n",
    "    \n",
    "    # Plot the fitted Gaussian\n",
    "    ax.plot(x_fit_inst, y_fit_inst, color='slategrey', linestyle='--', linewidth=2, label='Gaussian Fit (Average Speed)')\n",
    "    \n",
    "    # Calculate R^2 value for the Gaussian fit\n",
    "    residuals_inst = counts_inst_normalized - gaussian(bin_centers_inst, *params_inst)\n",
    "    ss_res_inst = np.sum(residuals_inst**2)\n",
    "    ss_tot_inst = np.sum((counts_inst_normalized - np.mean(counts_inst_normalized))**2)\n",
    "    r_squared_inst = 1 - (ss_res_inst / ss_tot_inst)\n",
    "\n",
    "    # Fit log-normal to the average instantaneous velocity\n",
    "    shape, loc, scale = lognorm.fit(avg_inst_velocity, floc=0)  # shape is the shape parameter\n",
    "\n",
    "    # Generate y values for the fitted log-normal\n",
    "    y_fit_lognorm = lognormal(x_fit_inst, shape, loc, scale)  # Correct number of arguments\n",
    "\n",
    "    # Plot the fitted log-normal\n",
    "    ax.plot(x_fit_inst, y_fit_lognorm, color='teal', linestyle='--', linewidth=2, label='Log-normal Fit (Average Speed)')\n",
    "    \n",
    "    # Calculate R^2 value for the log-normal fit\n",
    "    residuals_lognorm = counts_inst_normalized - lognormal(bin_centers_inst, shape, loc, scale)  # Correct number of arguments\n",
    "    ss_res_lognorm = np.sum(residuals_lognorm**2)\n",
    "    ss_tot_lognorm = np.sum((counts_inst_normalized - np.mean(counts_inst_normalized))**2)\n",
    "    r_squared_lognorm = 1 - (ss_res_lognorm / ss_tot_lognorm)\n",
    "    \n",
    "    # Calculate normalized counts for overall average velocities\n",
    "    counts_avg, bins_avg = np.histogram(avg_velocity, bins=bins_smaller)\n",
    "    counts_avg_normalized = counts_avg / counts_avg.sum()\n",
    "    ax.hist(bins_avg[:-1], bins=bins_smaller, weights=counts_avg_normalized, color='tan', alpha=0.5, label='Average Velocity')\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel('Velocity (μm/min)', fontsize=20)\n",
    "    plt.ylabel('Fraction of Counts', fontsize=20)\n",
    "    plt.title('Normalized Distribution of Average Speed and Average Velocity', fontsize=20)\n",
    "    \n",
    "    # Customize tick parameters\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=8)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(fontsize=15)\n",
    "    \n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save fitting parameters to CSV file\n",
    "    filepath = data_folder + '\\\\fit_velocity_parameters.csv'\n",
    "    if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "\n",
    "    with open(filepath, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "    save_fit_velocity_parameters_as_csv(params_inst, r_squared_inst, [shape, loc, scale], r_squared_lognorm, \n",
    "                                        filepath)\n",
    "\n",
    "    # Save as SVG\n",
    "    save_path = data_folder + '\\\\track_velocities.svg'\n",
    "    if os.path.exists(save_path):\n",
    "        os.remove(save_path)\n",
    "    fig.savefig(save_path, format='svg', bbox_inches='tight')\n",
    "    # print(f\"Plot saved as SVG at: {save_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "\n",
    "def fit_and_plot_sinuosity_distribution(data_folder, tp_trajectories_final):\n",
    "    \"\"\"\n",
    "    Plots the inverse sinuosity distribution and fits power-law and exponential decay curves.\n",
    "    \n",
    "    Parameters:\n",
    "    - tp_trajectories_final (pd.DataFrame): DataFrame containing track data with 'sinuosity' column.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Fit results containing parameters, errors, and R² values for power-law and exponential fits.\n",
    "    \"\"\"\n",
    "    # Convert sinuosity values to the desired units\n",
    "    inverse_sinuosity = 1 / tp_trajectories_final['sinuosity']\n",
    "    \n",
    "    # Define histogram bins from 0 to a maximum value\n",
    "    nbins = 20\n",
    "    bins = np.linspace(0, 1, nbins)  # Adjust the upper limit based on your data range\n",
    "    counts, bin_edges = np.histogram(inverse_sinuosity, bins=bins, density=False)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    normalized_counts = counts / np.sum(counts) if np.sum(counts) > 0 else counts\n",
    "\n",
    "    # Fit results dictionary to store parameters, errors, and R² values\n",
    "    fit_results = {\n",
    "        \"power_law\": {\"params\": None, \"errors\": None, \"r2\": None},\n",
    "        \"exponential\": {\"params\": None, \"errors\": None, \"r2\": None}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Fit power-law to non-zero counts\n",
    "        popt_power, pcov_power = curve_fit(power_law, bin_centers[counts > 0], normalized_counts[counts > 0], maxfev=10000)\n",
    "        perr_power = np.sqrt(np.diag(pcov_power))  # Standard deviation errors\n",
    "\n",
    "        # Calculate R² for power-law fit\n",
    "        power_pred = power_law(bin_centers, *popt_power)\n",
    "        ss_res_power = np.sum((normalized_counts - power_pred) ** 2)\n",
    "        ss_tot_power = np.sum((normalized_counts - np.mean(normalized_counts)) ** 2)\n",
    "        r2_power = 1 - (ss_res_power / ss_tot_power)\n",
    "\n",
    "        # Store power-law fit results\n",
    "        fit_results[\"power_law\"][\"params\"] = popt_power\n",
    "        fit_results[\"power_law\"][\"errors\"] = perr_power\n",
    "        fit_results[\"power_law\"][\"r2\"] = r2_power\n",
    "\n",
    "        # Fit exponential decay to non-zero counts\n",
    "        popt_exp, pcov_exp = curve_fit(exponential_decay, bin_centers[counts > 0], normalized_counts[counts > 0], maxfev=10000)\n",
    "        perr_exp = np.sqrt(np.diag(pcov_exp))  # Standard deviation errors\n",
    "\n",
    "        # Calculate R² for exponential fit\n",
    "        exp_pred = exponential_decay(bin_centers, *popt_exp)\n",
    "        ss_res_exp = np.sum((normalized_counts - exp_pred) ** 2)\n",
    "        ss_tot_exp = np.sum((normalized_counts - np.mean(normalized_counts)) ** 2)\n",
    "        r2_exp = 1 - (ss_res_exp / ss_tot_exp)\n",
    "\n",
    "        # Store exponential fit results\n",
    "        fit_results[\"exponential\"][\"params\"] = popt_exp\n",
    "        fit_results[\"exponential\"][\"errors\"] = perr_exp\n",
    "        fit_results[\"exponential\"][\"r2\"] = r2_exp\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error fitting curves: {e}\")\n",
    "\n",
    "    # Plot Inverse Sinuosity Distribution\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plot normalized histogram\n",
    "    ax.bar(bin_centers, normalized_counts, width=np.diff(bin_edges), color='lightcoral', edgecolor='k', \n",
    "           alpha=0.7, label='Normalized Inverse Sinuosity Histogram')\n",
    "\n",
    "    # Plot power-law fit\n",
    "    if fit_results[\"power_law\"][\"params\"] is not None:\n",
    "        ax.plot(bin_centers, power_law(bin_centers, *fit_results[\"power_law\"][\"params\"]), color='olive',  marker='o',\n",
    "                label=f'Power-law Fit: y = {fit_results[\"power_law\"][\"params\"][0]:.2f} *x^{{{fit_results[\"power_law\"][\"params\"][1]:.2f}}}')\n",
    "\n",
    "    # Plot exponential fit\n",
    "    if fit_results[\"exponential\"][\"params\"] is not None:\n",
    "        ax.plot(bin_centers, exponential_decay(bin_centers, *fit_results[\"exponential\"][\"params\"]), \n",
    "                color='slateblue', marker='o', \n",
    "                label=f'Exponential Fit: y = {fit_results[\"exponential\"][\"params\"][0]:.2f} *exp(-{fit_results[\"exponential\"][\"params\"][1]:.2f} * x)')\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_title('Inverse Sinuosity Distribution with Fitted Curves' , fontsize=20)\n",
    "    ax.set_xlabel('Inverse Sinuosity', fontsize=20)\n",
    "    ax.set_ylabel('Counts per Bin / Total Bins', fontsize=20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=8)\n",
    "    ax.grid(False)\n",
    "    ax.legend(fontsize=16)\n",
    "\n",
    "    # Save fitting parameters to CSV file\n",
    "    def save_fit_parameters_as_csv(fit_results, filepath):\n",
    "        \"\"\"\n",
    "        Saves the fit results (parameters, errors, R²) to a CSV file.\n",
    "        \n",
    "        Args:\n",
    "          fit_results (dict): Dictionary containing fit results.\n",
    "          filepath (str): Path to the CSV file for saving.\n",
    "        \"\"\"\n",
    "        with open(filepath, 'w', newline='') as csvfile:\n",
    "          writer = csv.writer(csvfile)\n",
    "          writer.writerow(['Fit Type', 'Parameter 1', 'Error 1', 'Parameter 2', 'Error 2', 'R²'])\n",
    "          writer.writerow(['Power Law', *fit_results['power_law']['params'], *fit_results['power_law']\n",
    "                           ['errors'], fit_results['power_law']['r2']])\n",
    "          writer.writerow(['Exponential Decay', *fit_results['exponential']['params'], *fit_results\n",
    "                           ['exponential']['errors'], fit_results['exponential']['r2']])\n",
    "\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "\n",
    "        with open(filepath, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(fit_results)\n",
    "    \n",
    "    save_fit_parameters_as_csv(fit_results, data_folder + '\\\\fit_sinuosity_parameters.csv')\n",
    "\n",
    "\n",
    "    # Save plot as SVG\n",
    "    save_path = data_folder + '\\\\inverse_sinuosity_distribution.svg'\n",
    "    \n",
    "    # Delete the old figure if it exists (combined logic)\n",
    "    if os.path.exists(save_path):\n",
    "        os.remove(save_path)\n",
    "    \n",
    "    # Save the new figure\n",
    "    fig.savefig(save_path, format='svg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    # return fit_results\n",
    "\n",
    "\n",
    "def analyze_trajectory_data(data_folder, file_name, resolution, frame_interval, kernel_size=11, \n",
    "                            min_frames=10, diameter=5, minmass=1.0, separation=15, search_range=5):\n",
    "    \"\"\"\n",
    "    Analyze trajectory data from raw images by processing optical flow, detecting and tracking particles, \n",
    "    and plotting various distributions.\n",
    "\n",
    "    Parameters:\n",
    "    - data_folder (str): Path to the folder containing raw image files.\n",
    "    - file_name (str): Name of the raw image file to analyze.\n",
    "    - resolution (float): Resolution in px/um.\n",
    "    - frame_interval (float): Time interval per frame in seconds.\n",
    "    - kernel_size (int): Kernel size for optical flow processing (default is 11).\n",
    "    - min_frames (int): Minimum number of frames for tracks to be retained (default is 10).\n",
    "    - diameter (float): Diameter of the particles for detection (default is 5).\n",
    "    - minmass (float): Minimum mass for particle detection (default is 1.0).\n",
    "    - separation (float): Minimum separation between detected particles (default is 15).\n",
    "    - search_range (float): Search range for particle tracking (default is 5).\n",
    "\n",
    "    Returns:\n",
    "    - None: Saves plots and fits to specified paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Construct the full image path\n",
    "        print(\"Step 1: Constructing the full image path...\")\n",
    "        image_path = os.path.join(data_folder, file_name)  # Ensure both parts are strings\n",
    "        print(f\"Image path: {image_path}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error in Step 1: Constructing the full image path. Details: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the image\n",
    "        img = tiff.imread(image_path)\n",
    "        print(\"Step 2: Image loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error in Step 2: Image file not found at {image_path}.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error in Step 2: Loading the image. Details: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Process optical flow and track particles\n",
    "        print(\"Step 3: Processing optical flow and tracking particles...\")\n",
    "        cluster, mask, n_frames = process_optical_flow(data_folder, file_name)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error in Step 3: Processing optical flow. Details: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Detect and track particles\n",
    "        print(\"Step 4: Detecting and tracking particles...\")\n",
    "        tp_trajectories = detect_and_track_particles(cluster, mask, n_frames, diameter=diameter, \n",
    "                                                     minmass=minmass, separation=separation, search_range=search_range)\n",
    "        print(f\"Particles detected and tracked. Total trajectories: {len(tp_trajectories)}.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error in Step 4: Detecting and tracking particles. Details: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Filter out tracks with less than the specified number of frames\n",
    "        print(\"Step 5: Filtering tracks...\")\n",
    "        tp_trajectories_pruned = tp.filter_stubs(tp_trajectories, min_frames)\n",
    "        print(f\"Tracks filtered. Remaining trajectories: {len(tp_trajectories_pruned)}.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error in Step 5: Filtering tracks. Details: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract metrics from the tracks\n",
    "        print(\"Step 6: Calculating track metrics...\")\n",
    "        tp_trajectories_final, track_metrics = calculate_track_metrics(tp_trajectories_pruned, resolution, frame_interval)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error in Step 6: Calculating track metrics. Details: {e}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Calculate angular differences\n",
    "        print(\"Step 7a : Calculating angular differences...\")\n",
    "        ang_diff_process = calculate_angular_differences_process(tp_trajectories_final, data_folder, save_path = data_folder+'\\\\angles.csv')\n",
    "    \n",
    "        # Plot the angular differences\n",
    "        print(\"Step 7b : Plotting angular differences...\")\n",
    "        plot_angular_differences_process(ang_diff_process, tp_trajectories_final, save_path = data_folder+'\\\\relative_angles.svg')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Visualize and save trajectories\n",
    "    # visualize_and_save_trajectories(tp_trajectories_final, img, data_folder)\n",
    "    # plot_particle_trajectories(tp_trajectories_final, data_folder, colormap_name='magma')\n",
    "\n",
    "    # Plotting all the track property distributions\n",
    "    # analyze_track_duration(data_folder, tp_trajectories_final)\n",
    "    # plot_track_total_distance_distribution(data_folder, tp_trajectories_final)\n",
    "    # fit_and_plot_track_displacement_distribution(data_folder, tp_trajectories_final)\n",
    "    # plot_velocity_distributions(data_folder, tp_trajectories_final)\n",
    "    # fit_and_plot_sinuosity_distribution(data_folder, tp_trajectories_final)\n",
    "\n",
    "\n",
    "    # Save DataFrames to CSV\n",
    "    save_path1 = data_folder + '\\\\tp_trajectories_final.csv'\n",
    "    if os.path.exists(save_path1):\n",
    "        os.remove(save_path1)\n",
    "    tp_trajectories_final.to_csv(save_path1, index=True)\n",
    "    \n",
    "    save_path2 = data_folder + '\\\\track_metrics.csv'\n",
    "    if os.path.exists(save_path2):\n",
    "        os.remove(save_path2)\n",
    "    track_metrics.to_csv(save_path2, index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_numeric(value):\n",
    "    \"\"\"\n",
    "    Extracts numeric value from a given input. If the input is not numeric,\n",
    "    it will return None.\n",
    "\n",
    "    Args:\n",
    "    - value: The input value to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - float or None: The extracted numeric value, or None if the input is not valid.\n",
    "    \"\"\"\n",
    "    # Convert to string and strip any leading/trailing whitespace\n",
    "    value_str = str(value).strip()\n",
    "\n",
    "    # Use regex to find numeric values (including decimals)\n",
    "    match = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", value_str)\n",
    "\n",
    "    if match:\n",
    "        return float(match[0])  # Return the first found numeric value\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_file_names_and_params_by_div(sheet_path, div_value, sheet_name='glass'):\n",
    "    \"\"\"\n",
    "    Get all 'file name', 'resolution', and 'frame interval' values from the specified Excel sheet where div \n",
    "    equals the given value.\n",
    "\n",
    "    Args:\n",
    "    - sheet_path: Path to the Excel sheet containing imaging details.\n",
    "    - div_value: The 'div' value to filter the Excel sheet.\n",
    "    - sheet_name: The sheet name in the Excel file (default: 'glass').\n",
    "\n",
    "    Returns:\n",
    "    - params: A list of tuples containing ('file name', 'resolution', 'frame interval') where div equals div_value.\n",
    "    \"\"\"\n",
    "    # Load the Excel sheet\n",
    "    df_sheet = pd.read_excel(sheet_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Filter based on the 'div' value\n",
    "    df_sheet_filtered = df_sheet[df_sheet['div'] == div_value]\n",
    "\n",
    "    # Get the parameters\n",
    "    params = []\n",
    "    for index in df_sheet_filtered.index:\n",
    "        # Extract file name\n",
    "        file_name = df_sheet_filtered['file name'][index]\n",
    "\n",
    "        # Extract frame interval\n",
    "        interval = df_sheet_filtered['frame interval'][index]\n",
    "        integer_interval_value = extract_numeric(interval)\n",
    "\n",
    "        # Extract resolution\n",
    "        resolution = df_sheet_filtered['resolution'][index]\n",
    "        integer_resolution_value = extract_numeric(resolution)\n",
    "\n",
    "        # Append to params\n",
    "        params.append((file_name, integer_resolution_value, integer_interval_value))\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "\n",
    "def check_level_2_subfolders_by_div_and_params(main_folder, sheet_path, div_value, resolution_range, frame_interval_range):\n",
    "    \"\"\"\n",
    "    Check all level-2 subfolder names under level-1 subfolders that match the specified div value,\n",
    "    and verify if their corresponding resolution and frame interval match the criteria.\n",
    "\n",
    "    Args:\n",
    "    - main_folder: Path to the main folder containing level-1 subfolders.\n",
    "    - sheet_path: Path to the Excel sheet containing imaging details.\n",
    "    - div_value: The 'div' value to filter the Excel sheet.\n",
    "    - resolution_range: The resolution range to match (as a tuple or list).\n",
    "    - frame_interval_range: The frame interval range to match (as a tuple or list).\n",
    "    \"\"\"\n",
    "    # Get the file names and corresponding parameters that match the div value\n",
    "    params = get_file_names_and_params_by_div(sheet_path, div_value)\n",
    "\n",
    "    if not params:\n",
    "        print(f\"No parameters found for div = {div_value}.\")\n",
    "        return\n",
    "\n",
    "    # Create the expected level-1 subfolder name\n",
    "    level_1_subfolder_name = f\"div{div_value}\"\n",
    "\n",
    "    # Loop through each level-1 subfolder\n",
    "    for subfolder in os.listdir(main_folder):\n",
    "        subfolder_path = os.path.join(main_folder, subfolder)\n",
    "\n",
    "        # Check if it's a directory (level-1) and matches the expected name\n",
    "        if os.path.isdir(subfolder_path) and subfolder == level_1_subfolder_name:\n",
    "            # Loop through each level-2 subfolder\n",
    "            for level_2_subfolder in os.listdir(subfolder_path):\n",
    "                level_2_subfolder_path = os.path.join(subfolder_path, level_2_subfolder)\n",
    "\n",
    "                if os.path.isdir(level_2_subfolder_path):\n",
    "                    # Check if this level-2 folder corresponds to any file name\n",
    "                    for file_name, resolution, frame_interval in params:\n",
    "                        if file_name in level_2_subfolder:\n",
    "                            print(f\"Analyzing Level-2 Subfolder: {level_2_subfolder}\")\n",
    "\n",
    "                            # Verify if resolution and frame interval meet the criteria\n",
    "                            if ((resolution_range is None or (resolution is not None and \n",
    "                                                              resolution_range[0] <= resolution <= resolution_range[1])) and\n",
    "                                (frame_interval_range is None or (frame_interval is not None and frame_interval_range[0] <= \n",
    "                                                                  frame_interval <= frame_interval_range[1]))):\n",
    "                                \n",
    "                                print(\"Criteria Met\")\n",
    "                                \n",
    "                                # Search for files ending with '_jttr_blch_corr.tiff'\n",
    "                                found_file = None\n",
    "                                for file in os.listdir(level_2_subfolder_path):\n",
    "                                    if file.endswith('_jttr_blch_corr.tiff'):\n",
    "                                        found_file = file\n",
    "                                        break\n",
    "                                \n",
    "                                # If a file is found, proceed with analysis\n",
    "                                if found_file:\n",
    "                                    folder_path = convert_to_code_path(subfolder_path + '\\\\' + level_2_subfolder)\n",
    "                                    try:\n",
    "                                        analyze_trajectory_data(folder_path, found_file, resolution, frame_interval, min_frames=8)\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"Error analyzing data in {level_2_subfolder}: {e}\")\n",
    "                                else:\n",
    "                                    print(\"No '_jttr_blch_corr.tiff' file found in this folder.\")\n",
    "                            else:\n",
    "                                print(\"Criteria Not Met\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def analyze_all_level_2_folders_by_div(main_folder, sheet_path, div_value, resolution_range, frame_interval_range):\n",
    "    \"\"\"\n",
    "    Analyze all level-2 folders under level-1 subfolders based on the specified div value,\n",
    "    resolution range, and frame interval range.\n",
    "\n",
    "    Args:\n",
    "    - main_folder: Path to the main folder containing level-1 subfolders.\n",
    "    - sheet_path: Path to the Excel sheet containing imaging details.\n",
    "    - div_value: The 'div' value to filter the Excel sheet.\n",
    "    - resolution_range: The resolution range to match (as a tuple or list).\n",
    "    - frame_interval_range: The frame interval range to match (as a tuple or list).\n",
    "    \"\"\"\n",
    "    # Check level-2 subfolders by div and parameters\n",
    "    check_level_2_subfolders_by_div_and_params(main_folder, sheet_path, div_value, resolution_range, frame_interval_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70ba05-3007-415f-b2fb-c74ff6c92903",
   "metadata": {},
   "source": [
    "## Practice example for a DIV stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01d90bac-0839-4d43-8f15-3f9b4826b702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 379: 83 trajectories present.\n",
      "Particles detected and tracked. Total trajectories: 30558.\n",
      "Step 5: Filtering tracks...\n",
      "Tracks filtered. Remaining trajectories: 2090.\n",
      "Step 6: Calculating track metrics...\n",
      "Trajectories and track properties calculated and stored.\n",
      "Step 7a : Calculating angular differences...\n",
      "Step 7b : Plotting angular differences...\n"
     ]
    }
   ],
   "source": [
    "# Example call\n",
    "main_folder = r'E:\\Spandan\\2D_Neurons_Paper\\Glass'\n",
    "sheet_path = r'E:\\Spandan\\Kate\\NEURON MOVIES\\tifNotes.xlsx'\n",
    "div_value = 11\n",
    "resolution_range = (2.6, 2.9)\n",
    "frame_interval_range = (1.95, 2.05)\n",
    "\n",
    "analyze_all_level_2_folders_by_div(main_folder, sheet_path, div_value, resolution_range, frame_interval_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad6990-1c4b-4ed4-88c5-99b1ff4e0b25",
   "metadata": {},
   "source": [
    "## Main function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ca3bb41-a432-4bda-893e-0d470ffe4a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 124: 73 trajectories present.\n",
      "Particles detected and tracked. Total trajectories: 9405.\n",
      "Step 5: Filtering tracks...\n",
      "Tracks filtered. Remaining trajectories: 768.\n",
      "Step 6: Calculating track metrics...\n",
      "Trajectories and track properties calculated and stored.\n",
      "Step 7a : Calculating angular differences...\n",
      "Step 7b : Plotting angular differences...\n",
      "Analyzing Level-2 Subfolder: control_2021_12_06_div7neurons_dish1_15mm_40x_neuron3_media\n",
      "Criteria Met\n",
      "Step 1: Constructing the full image path...\n",
      "Image path: E:\\\\Spandan\\\\2D_Neurons_Paper\\\\Glass\\\\div7\\\\control_2021_12_06_div7neurons_dish1_15mm_40x_neuron3_media\\div7neurons_dish1_15mm_40x_neuron3_media_jttr_blch_corr.tiff\n",
      "Step 2: Image loaded successfully.\n",
      "Step 3: Processing optical flow and tracking particles...\n",
      "Error: [WinError 3] The system cannot find the path specified: 'E:\\\\\\\\Spandan\\\\\\\\2D_Neurons_Paper\\\\\\\\Glass\\\\\\\\div7\\\\\\\\control_2021_12_06_div7neurons_dish1_15mm_40x_neuron3_media\\\\Op_flow'\n",
      "Error analyzing data in control_2021_12_06_div7neurons_dish1_15mm_40x_neuron3_media: Error in Step 3: Processing optical flow. Details: cannot unpack non-iterable NoneType object\n",
      "Analyzing div8 with DIV value: 8\n",
      "Analyzing Level-2 Subfolder: control_2019_03_24_div8cortex_timelapse1_noStim\n",
      "Criteria Not Met\n",
      "Analyzing Level-2 Subfolder: control_2021_04_12_MAX_6wellPlate_timelapse1_100x\n",
      "Criteria Not Met\n",
      "Analyzing Level-2 Subfolder: control_2021_04_12_MAX_6wellPlate_timelapse2_100x\n",
      "Criteria Not Met\n",
      "Analyzing Level-2 Subfolder: control_2021_04_12_MAX_6wellPlate_timelapse3_100x\n",
      "Criteria Not Met\n"
     ]
    }
   ],
   "source": [
    "def analyze_all_divs(main_folder, sheet_path, resolution_range, frame_interval_range):\n",
    "    \"\"\"\n",
    "    Analyze all level-1 folders corresponding to different DIV values.\n",
    "\n",
    "    Parameters:\n",
    "    - main_folder: Path to the main folder containing level-1 subfolders.\n",
    "    - sheet_path: Path to the Excel file with additional data.\n",
    "    - resolution_range: Tuple containing the range of resolutions to filter.\n",
    "    - frame_interval_range: Tuple containing the range of frame intervals to filter.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loop through each level-1 subfolder\n",
    "    for subfolder in os.listdir(main_folder):\n",
    "        subfolder_path = os.path.join(main_folder, subfolder)\n",
    "\n",
    "        # Check if it's a directory (level-1)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            # Use regex to extract the DIV value from the folder name\n",
    "            match = re.search(r'div(\\d+)', subfolder, re.IGNORECASE)\n",
    "            if match:\n",
    "                div_value = int(match.group(1))  # Convert extracted string to integer\n",
    "                print(f\"Analyzing {subfolder} with DIV value: {div_value}\")\n",
    "\n",
    "                # Call your analysis function\n",
    "                analyze_all_level_2_folders_by_div(main_folder, sheet_path, div_value, resolution_range, frame_interval_range)\n",
    "\n",
    "# Example usage\n",
    "main_folder = r'E:\\Spandan\\2D_Neurons_Paper\\Glass'\n",
    "sheet_path = r'E:\\Spandan\\Kate\\NEURON MOVIES\\tifNotes.xlsx'\n",
    "resolution_range = (2.6, 2.9)\n",
    "frame_interval_range = (1.8, 2.2)\n",
    "\n",
    "analyze_all_divs(main_folder, sheet_path, resolution_range, frame_interval_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186fae1-b55b-4eb1-a583-fac2216292bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
